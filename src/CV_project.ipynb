{"cells":[{"cell_type":"markdown","source":["# **Dataset loading**"],"metadata":{"id":"fuBGU_ddPASi"}},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"ykB19emZWcIK"},"outputs":[],"source":["!pip install kaggle\n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"yOYk95lVWcIN"},"outputs":[],"source":["!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json\n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"DiNTMZf9WcIO"},"outputs":[],"source":["!kaggle datasets download -d kspsvln/pre-processed-image-forgery-dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"oU_aUv3lWcIO"},"outputs":[],"source":["!unzip pre-processed-image-forgery-dataset.zip -d ./patches"]},{"cell_type":"markdown","metadata":{"id":"QY_DLIMF843J"},"source":["# **Packages**"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6496,"status":"ok","timestamp":1727345373287,"user":{"displayName":"KSPSVLN Siddardha Kumar Kavuri","userId":"12765768515309216469"},"user_tz":-330},"gather":{"logged":1724028700260},"id":"3MzUf28Z6gIt","outputId":"37852c90-f415-44ec-873d-17167062c897"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.24.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Requirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (0.12.1)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (0.20.3)\n","Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.3)\n","Collecting imblearn\n","  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n","Requirement already satisfied: kiwisolver in /usr/local/lib/python3.10/dist-packages (1.4.7)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (3.1.4)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (2.8.2)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (2024.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (1.16.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n","Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.35.1)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2024.9.20)\n","Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (24.1)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (0.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n","Installing collected packages: imblearn\n","Successfully installed imblearn-0.0\n"]}],"source":["!pip install torch torchvision joblib scikit-image opencv-python cycler graphviz imbalanced-learn imblearn kiwisolver matplotlib numpy pandas pyparsing python-dateutil pytz scikit-learn scipy seaborn six"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3671,"status":"ok","timestamp":1727345376953,"user":{"displayName":"KSPSVLN Siddardha Kumar Kavuri","userId":"12765768515309216469"},"user_tz":-330},"gather":{"logged":1724028709048},"id":"u_2Ql9VNszS5","jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"outputId":"7ee88356-b6b9-4942-ff29-802a9a36eacc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.26.4)\n"]}],"source":["!pip install opencv-python-headless opencv-contrib-python"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3570,"status":"ok","timestamp":1727345380516,"user":{"displayName":"KSPSVLN Siddardha Kumar Kavuri","userId":"12765768515309216469"},"user_tz":-330},"gather":{"logged":1724028713478},"id":"0jnERUtxszS6","jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"outputId":"a754ab10-ca49-43e5-bf7c-6974f075db5e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://download.pytorch.org/whl/cu118\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}],"source":["!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"]},{"cell_type":"code","execution_count":8,"metadata":{"tags":[],"id":"sxUH-XOZWcIR","outputId":"507ddc58-833c-4fb7-fe09-4eef506f52f2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727345406802,"user_tz":-330,"elapsed":26294,"user":{"displayName":"KSPSVLN Siddardha Kumar Kavuri","userId":"12765768515309216469"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n","Get:6 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n","Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,001 kB]\n","Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n","Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,348 kB]\n","Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,156 kB]\n","Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,586 kB]\n","Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,113 kB]\n","Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,317 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,191 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,594 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,444 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [111 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [36.9 kB]\n","Fetched 26.3 MB in 7s (4,019 kB/s)\n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following NEW packages will be installed:\n","  libgl1-mesa-glx\n","0 upgraded, 1 newly installed, 0 to remove and 52 not upgraded.\n","Need to get 5,584 B of archives.\n","After this operation, 74.8 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgl1-mesa-glx amd64 23.0.4-0ubuntu1~22.04.1 [5,584 B]\n","Fetched 5,584 B in 0s (12.4 kB/s)\n","Selecting previously unselected package libgl1-mesa-glx:amd64.\n","(Reading database ... 123605 files and directories currently installed.)\n","Preparing to unpack .../libgl1-mesa-glx_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n"]}],"source":["!apt-get update\n","!apt-get install -y libgl1-mesa-glx"]},{"cell_type":"code","execution_count":9,"metadata":{"gather":{"logged":1724051228212},"id":"8c2tiXc469hs","executionInfo":{"status":"ok","timestamp":1727345416187,"user_tz":-330,"elapsed":9390,"user":{"displayName":"KSPSVLN Siddardha Kumar Kavuri","userId":"12765768515309216469"}}},"outputs":[],"source":["from typing import Dict\n","from torch import Tensor, stack\n","import torch\n","import math\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import torch.optim as optim\n","import torch.utils.data\n","from torch.optim.lr_scheduler import StepLR\n","from torch.autograd import Variable\n","import time\n","import pandas as pd\n","import torchvision.transforms as transforms\n","from torchvision import datasets\n","import glob\n","import cv2\n","from skimage.util import view_as_windows\n","import os\n","from skimage import io\n","\n","from sklearn import svm\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from matplotlib.colors import ListedColormap\n","import seaborn as sn"]},{"cell_type":"markdown","metadata":{"id":"3U23mBUfsmr2"},"source":["# **Patch Extraction**"]},{"cell_type":"code","execution_count":null,"metadata":{"gather":{"logged":1724031149362},"id":"xnFlO4c2BUhT"},"outputs":[],"source":["import os\n","from glob import glob\n","import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from skimage.metrics import structural_similarity\n","\n","\n","def find_mask(sp_pic, au_pic_dict):\n","    \"\"\"\n","    Extracts and saves the mask (ground truth) for the given tampered image.\n","    :param sp_pic: Tampered image\n","    :param au_pic_dict: Dictionary with keys the name of the tampered image and values its path.\n","    \"\"\"\n","    background_index = [13, 21]  # indices of background image in the tamoered image name\n","    save_name = sp_pic.split(os.sep)[-1][:-4]  # name of the mask\n","    sp_name = sp_pic.split(os.sep)[-1][background_index[0]:background_index[1]]\n","    if sp_name in au_pic_dict.keys():\n","        au_pic = au_pic_dict[sp_name]\n","        au_image = plt.imread(au_pic)\n","        sp_image = plt.imread(sp_pic)\n","        if sp_image.shape == au_image.shape:\n","            # convert images to grayscale\n","            gray_au_image = cv2.cvtColor(au_image, cv2.COLOR_BGR2GRAY)\n","            gray_sp_image = cv2.cvtColor(sp_image, cv2.COLOR_BGR2GRAY)\n","            # get the difference of the 2 grayscale images\n","            (_, diff) = structural_similarity(gray_au_image, gray_sp_image, full=True)\n","            diff = cv2.medianBlur(diff, 1)\n","            # make background black and tampered area white\n","            mask = np.ones_like(diff)\n","            mask[diff < 0.98] = 1\n","            mask[diff >= 0.98] = 0\n","            mask = (mask * 255).astype(\"uint8\")\n","            cv2.imwrite('drive/MyDrive/summer/Final/data/masks/' + save_name + '_gt.png', mask)\n","\n","\n","def extract_masks():\n","    \"\"\"\n","    Extracts and saves all the masks.\n","    \"\"\"\n","    # create the save directory\n","    save_dir = 'masks'\n","    if not os.path.exists(save_dir):\n","        os.makedirs('drive/MyDrive/summer/Final/data/'+save_dir)\n","    # else:\n","    #     delete_prev_images(save_dir)\n","\n","    # read the authentic and tampered images\n","    au_pic_list = glob('drive/MyDrive/summer/Final/CASIA2.0_revised' + os.sep + 'Au' + os.sep + '*')\n","    sp_pic_list = glob('drive/MyDrive/summer/Final/CASIA2.0_revised' + os.sep + 'Tp' + os.sep + '*')\n","\n","    au_index = [3, 6, 7, 12]  # indices of authetic image name\n","\n","    au_pic_dict = {\n","        au_pic.split(os.sep)[-1][au_index[0]:au_index[1]] + au_pic.split(os.sep)[-1][au_index[2]:au_index[3]]:\n","            au_pic for au_pic\n","        in au_pic_list}\n","    # extract the mask for every tampered image\n","    for _, Sp_pic in enumerate(sp_pic_list):\n","        find_mask(Sp_pic, au_pic_dict)\n","\n","\n","if __name__ == '__main__':\n","    extract_masks()"]},{"cell_type":"code","execution_count":null,"metadata":{"gather":{"logged":1724047728106},"id":"_8y8LjsXBcrR"},"outputs":[],"source":["import torchvision.transforms.functional as tf\n","import pandas as pd\n","import os\n","import numpy as np\n","from skimage.util import view_as_windows\n","from skimage import io\n","import PIL\n","\n","\n","def delete_prev_images(dir_name):\n","    \"\"\"\n","    Deletes all the file in a directory.\n","    :param dir_name: Directory name\n","    \"\"\"\n","    for the_file in os.listdir(dir_name):\n","        file_path = os.path.join(dir_name, the_file)\n","        try:\n","            if os.path.isfile(file_path):\n","                os.unlink(file_path)\n","        except Exception as e:\n","            print(e)\n","\n","\n","def check_and_reshape(image, input_mask):\n","    \"\"\"\n","    Gets an image reshapes it and returns it with its mask.\n","    :param image: The image\n","    :param input_mask: The mask of the image\n","    :returns: the image and its mask\n","    \"\"\"\n","    try:\n","        mask_x, mask_y = input_mask.shape\n","        mask = np.empty((mask_x, mask_y, 3))\n","        mask[:, :, 0] = input_mask\n","        mask[:, :, 1] = input_mask\n","        mask[:, :, 2] = input_mask\n","    except ValueError:\n","        mask = input_mask\n","    if image.shape == mask.shape:\n","        return image, mask\n","    elif image.shape[0] == mask.shape[1] and image.shape[1] == mask.shape[0]:\n","        mask = np.reshape(mask, (image.shape[0], image.shape[1], mask.shape[2]))\n","        return image, mask\n","    else:\n","        return image, mask\n","\n","\n","def extract_all_patches(image, window_shape, stride, num_of_patches, rotations, output_path, im_name, rep_num, mode):\n","    \"\"\"\n","    Extracts all the patches from an image.\n","    :param image: The image\n","    :param window_shape: The shape of the window (for example (128,128,3) in the CASIA2 dataset)\n","    :param stride: The stride of the patch extraction\n","    :param num_of_patches: The amount of patches to be extracted per image\n","    :param rotations: The amount of rotations divided equally in 360 degrees\n","    :param output_path: The output path where the patches will be saved\n","    :param im_name: The name of the image\n","    :param rep_num: The amount of repetitions\n","    :param mode: If we account rotations 'rot' or nor 'no_rot'\n","    \"\"\"\n","    non_tampered_windows = view_as_windows(image, window_shape, step=stride)\n","    non_tampered_patches = []\n","    for m in range(non_tampered_windows.shape[0]):\n","        for n in range(non_tampered_windows.shape[1]):\n","            non_tampered_patches += [non_tampered_windows[m][n][0]]\n","    # select random some patches, rotate and save them\n","    save_patches(non_tampered_patches, num_of_patches, mode, rotations, output_path, im_name, rep_num,\n","                 patch_type='authentic')\n","\n","\n","def save_patches(patches, num_of_patches, mode, rotations, output_path, im_name, rep_num, patch_type):\n","    \"\"\"\n","    Saves all the extracted patches to the output path.\n","    :param patches: The extracted patches\n","    :param num_of_patches: The amount of patches to be extracted per image\n","    :param mode: If we account rotations 'rot' or nor 'no_rot'\n","    :param rotations: The amount of rotations divided equally in 360 degrees\n","    :param output_path: The output path where the patches will be saved\n","    :param im_name: The name of the image\n","    :param rep_num: The amount of repetitions\n","    :param patch_type: The mask of the image\n","    \"\"\"\n","    inds = np.random.choice(len(patches), num_of_patches, replace=False)\n","    if mode == 'rot':\n","        for i, ind in enumerate(inds):\n","          image = patches[ind][0] if patch_type == 'tampered' else patches[ind]\n","          for angle in rotations:\n","            im_rt = PIL.Image.fromarray(np.uint8(image)).rotate(angle=angle, resample=PIL.Image.BILINEAR)\n","            im_rt.save(output_path + '/{0}/{1}_{2}_{3}_{4}.png'.format(patch_type, im_name, i, angle, rep_num))\n","    else:\n","        for i, ind in enumerate(inds):\n","            image = patches[ind][0] if patch_type == 'tampered' else patches[ind]\n","            io.imsave(output_path + '/{0}/{1}_{2}_{3}.png'.format(patch_type, im_name, i, rep_num), image)\n","\n","\n","def create_dirs(output_path):\n","    \"\"\"\n","    Creates the directories to the output path.\n","    :param output_path: The output path\n","    \"\"\"\n","    if not os.path.exists(output_path):\n","        os.makedirs(output_path)\n","        os.makedirs(output_path + '/authentic')\n","        os.makedirs(output_path + '/tampered')\n","    else:\n","        if os.path.exists(output_path + '/authentic'):\n","            delete_prev_images(output_path + '/authentic')\n","        else:\n","            os.makedirs(output_path + '/authentic')\n","        if os.path.exists(output_path + '/tampered'):\n","            delete_prev_images(output_path + '/tampered')\n","        else:\n","            os.makedirs(output_path + '/tampered')\n","\n","\n","class NotSupportedDataset(Exception):\n","    pass\n","\n","\n","def find_tampered_patches(image, im_name, mask, window_shape, stride, dataset, patches_per_image):\n","    \"\"\"\n","    Gets an image reshapes it and returns it with its mask.\n","    :param image: The image\n","    :param im_name: The name of the image\n","    :param mask: The mask of the image\n","    :param window_shape: The shape of the window (for example (128,128,3) in the CASIA2 dataset)\n","    :param stride: The stride of the patch extraction\n","    :param dataset: The name of the dataset\n","    :param patches_per_image: The amount of patches to be extracted per image\n","    :returns: the tampered patches and their amount\n","    \"\"\"\n","    # extract patches from images and masks\n","    patches = view_as_windows(image, window_shape, step=stride)\n","\n","    if dataset == 'casia2':\n","        mask_patches = view_as_windows(mask, window_shape, step=stride)\n","    else:\n","        raise NotSupportedDataset('The datasets supported are casia2 and nc16')\n","\n","    tampered_patches = []\n","    # find tampered patches\n","    for m in range(patches.shape[0]):\n","        for n in range(patches.shape[1]):\n","            im = patches[m][n][0]\n","            ma = mask_patches[m][n][0]\n","            num_zeros = (ma == 0).sum()\n","            num_ones = (ma == 255).sum()\n","            total = num_ones + num_zeros\n","            if dataset == 'casia2':\n","                if num_zeros <= 0.99 * total:\n","                    tampered_patches += [(im, ma)]\n","\n","    # if patches are less than the given number then take the minimum possible\n","    num_of_patches = patches_per_image\n","    if len(tampered_patches) < num_of_patches:\n","        print(\"Number of tampered patches for image {} is only {}\".format(im_name, len(tampered_patches)))\n","        num_of_patches = len(tampered_patches)\n","\n","    return tampered_patches, num_of_patches"]},{"cell_type":"code","execution_count":null,"metadata":{"gather":{"logged":1724044638894},"id":"5svCuC64q_t9"},"outputs":[],"source":["from glob import glob\n","from skimage import io\n","import os\n","import matplotlib.pyplot as plt\n","import warnings\n","\n","\n","warnings.filterwarnings('ignore')\n","# from src.patch_extraction.mask_extraction import extract_masks\n","\n","\n","class PatchExtractorCASIA:\n","    \"\"\"\n","    Patch extraction class\n","    \"\"\"\n","\n","    def __init__(self, input_path, output_path, patches_per_image=4, rotations=8, stride=8, mode='no_rot'):\n","        \"\"\"\n","        Initialize class\n","        :param patches_per_image: Number of samples to extract for each image\n","        :param rotations: Number of rotations to perform\n","        :param stride: Stride size to be used\n","        \"\"\"\n","        self.patches_per_image = patches_per_image\n","        self.stride = stride\n","        rots = [0, 90, 180, 270]\n","        self.rotations = rots[:rotations]\n","        self.mode = mode\n","        self.input_path = input_path\n","        self.output_path = output_path\n","\n","        # define the indices of the image names and read the authentic images\n","        self.background_index = [13, 21]\n","        au_index = [3, 6, 7, 12]\n","        au_pic_list = glob(self.input_path + os.sep + 'Au' + os.sep + '*')\n","        self.au_pic_dict = {\n","            au_pic.split(os.sep)[-1][au_index[0]:au_index[1]] + au_pic.split(os.sep)[-1][au_index[2]:au_index[3]]:\n","                au_pic for au_pic\n","            in au_pic_list}\n","\n","    def extract_authentic_patches(self, sp_pic, num_of_patches, rep_num):\n","        \"\"\"\n","        Extracts and saves the patches from the authentic image\n","        :param sp_pic: Name of tampered image\n","        :param num_of_patches: Number of patches to be extracted\n","        :param rep_num: Number of repetitions being done(just for the patch name)\n","        \"\"\"\n","        sp_name = sp_pic.split('/')[-1][self.background_index[0]:self.background_index[1]]\n","        if sp_name in self.au_pic_dict.keys():\n","            au_name = self.au_pic_dict[sp_name].split(os.sep)[-1].split('.')[0]\n","            # define window size\n","            window_shape = (128, 128, 3)\n","            au_pic = self.au_pic_dict[sp_name]\n","            au_image = plt.imread(au_pic)\n","            # extract all patches\n","            extract_all_patches(au_image, window_shape, self.stride, num_of_patches, self.rotations, self.output_path,\n","                                au_name, rep_num, self.mode)\n","\n","    def extract_patches(self):\n","        \"\"\"\n","        Main function which extracts all patches\n","        :return:\n","        \"\"\"\n","        create_dirs(self.output_path)\n","\n","        # define window shape\n","        window_shape = (128, 128, 3)\n","        tp_dir = self.input_path+'/Tp/'\n","        rep_num = 0\n","        # run for all the tampered images\n","        for f in os.listdir(tp_dir):\n","            try:\n","                rep_num += 1\n","                image = io.imread(tp_dir + f)\n","                im_name = f.split(os.sep)[-1].split('.')[0]\n","                # read mask\n","                mask = io.imread('drive/MyDrive/summer/Final/data/masks/' + im_name + '_gt.png')\n","                image, mask = check_and_reshape(image, mask)\n","\n","                # extract patches from images and masks\n","                tampered_patches, num_of_patches = find_tampered_patches(image, im_name, mask,\n","                                                                         window_shape, self.stride, 'casia2',\n","                                                                         self.patches_per_image)\n","                save_patches(tampered_patches, num_of_patches, self.mode, self.rotations, self.output_path, im_name,\n","                             rep_num, patch_type='tampered')\n","                self.extract_authentic_patches(tp_dir + f, num_of_patches, rep_num)\n","            except IOError as e:\n","                rep_num -= 1\n","                print(str(e))\n","            except IndexError:\n","                rep_num -= 1\n","                print('Mask and image have not the same dimensions')\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"gather":{"logged":1724035947344},"id":"dtZZgkH_3lr0"},"outputs":[],"source":["# CASIA Dataset\n","# mode='no_rot' for no rotations\n","pe = PatchExtractorCASIA(input_path='drive/MyDrive/summer/Final/CASIA2.0_revised', output_path='drive/MyDrive/summer/Final/data/patches_casia_with_rot',\n","                         patches_per_image=2, stride=128, rotations=4, mode='rot')\n","pe.extract_patches()"]},{"cell_type":"markdown","metadata":{"id":"AmvmJapo1PnK"},"source":["# **Custom Weights**"]},{"cell_type":"code","execution_count":16,"metadata":{"gather":{"logged":1724051024634},"id":"t7IVcpHJA_3q","executionInfo":{"status":"ok","timestamp":1727345863574,"user_tz":-330,"elapsed":639,"user":{"displayName":"KSPSVLN Siddardha Kumar Kavuri","userId":"12765768515309216469"}}},"outputs":[],"source":["from typing import Dict\n","\n","import numpy as np\n","from torch import Tensor, stack\n","\n","\n","def get_filters():\n","    \"\"\"\n","    Function that return the required high pass SRM filters for the first convolutional layer of our implementation\n","    :return: A pytorch Tensor containing the 30x3x5x5 filter tensor with type\n","    [number_of_filters, input_channels, height, width]\n","    \"\"\"\n","\n","    filters: Dict[str, Tensor] = {}\n","\n","    # 1st Order\n","    filters[\"11\"] = Tensor(np.array([[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, -1, 1, 0], [0, 0, 0, 0, 0],\n","                                      [0, 0, 0, 0, 0]]))\n","    filters[\"12\"] = Tensor(np.rot90(filters[\"1O1\"]).copy())\n","    filters[\"13\"] = Tensor(np.rot90(filters[\"1O2\"]).copy())\n","    filters[\"14\"] = Tensor(np.rot90(filters[\"1O3\"]).copy())\n","    filters[\"15\"] = Tensor(np.array([[0, 0, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, -1, 0, 0], [0, 0, 0, 0, 0],\n","                                      [0, 0, 0, 0, 0]]))\n","    filters[\"16\"] = Tensor(np.rot90(filters[\"1O5\"]).copy())\n","    filters[\"17\"] = Tensor(np.rot90(filters[\"1O6\"]).copy())\n","    filters[\"18\"] = Tensor(np.rot90(filters[\"1O7\"]).copy())\n","    # 2nd Order\n","    filters[\"21\"] = Tensor(np.array([[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 1, -2, 1, 0], [0, 0, 0, 0, 0],\n","                                      [0, 0, 0, 0, 0]]))\n","    filters[\"22\"] = Tensor(np.rot90(filters[\"2O1\"]).copy())\n","    filters[\"23\"] = Tensor(np.array([[0, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, -2, 0, 0], [0, 0, 0, 1, 0],\n","                                      [0, 0, 0, 0, 0]]))\n","    filters[\"24\"] = Tensor(np.rot90(filters[\"2O3\"]).copy())\n","    # 3rd Order\n","    filters[\"31\"] = Tensor(np.array([[0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 1, -3, 1, 0], [0, 0, 0, 0, 0],\n","                                      [0, 0, 0, 0, 0]]))\n","    filters[\"32\"] = Tensor(np.rot90(filters[\"3O1\"]).copy())\n","    filters[\"33\"] = Tensor(np.rot90(filters[\"3O2\"]).copy())\n","    filters[\"34\"] = Tensor(np.rot90(filters[\"3O3\"]).copy())\n","    filters[\"35\"] = Tensor(np.array([[0, 0, 0, 0, 0], [0, 1, 0, 1, 0], [0, 0, -3, 0, 0], [0, 1, 0, 0, 0],\n","                                      [0, 0, 0, 0, 0]]))\n","    filters[\"36\"] = Tensor(np.rot90(filters[\"3O5\"]).copy())\n","    filters[\"37\"] = Tensor(np.rot90(filters[\"3O6\"]).copy())\n","    filters[\"38\"] = Tensor(np.rot90(filters[\"3O7\"]).copy())\n","    # 3x3 SQUARE\n","    filters[\"3S\"] = Tensor(np.array([[0, 0, 0, 0, 0], [0, -1, 2, -1, 0], [0, 2, -4, 2, 0], [0, -1, 2, -1, 0],\n","                                       [0, 0, 0, 0, 0]]))\n","    # 3x3 EDGE\n","    filters[\"E1\"] = Tensor(np.array([[0, 0, 0, 0, 0], [0, -1, 2, -1, 0], [0, 2, -4, 2, 0], [0, 0, 0, 0, 0],\n","                                        [0, 0, 0, 0, 0]]))\n","    filters[\"E2\"] = Tensor(np.rot90(filters[\"3x3E1\"]).copy())\n","    filters[\"E3\"] = Tensor(np.rot90(filters[\"3x3E2\"]).copy())\n","    filters[\"E4\"] = Tensor(np.rot90(filters[\"3x3E3\"]).copy())\n","    # 5X5 EDGE\n","    filters[\"E1\"] = Tensor(np.array([[-1, 2, -2, 2, -1], [2, -6, 8, -6, 2], [-2, 8, -12, 8, -2],\n","                                        [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]))\n","    filters[\"E2\"] = Tensor(np.rot90(filters[\"5x5E1\"]).copy())\n","    filters[\"E3\"] = Tensor(np.rot90(filters[\"5x5E2\"]).copy())\n","    filters[\"E4\"] = Tensor(np.rot90(filters[\"5x5E3\"]).copy())\n","    # 5x5 SQUARE\n","    filters[\"5S\"] = Tensor(np.array([[-1, 2, -2, 2, -1], [2, -6, 8, -6, 2], [-2, 8, -12, 8, -2],\n","                                       [2, -6, 8, -6, 2], [-1, 2, -2, 2, -1]]))\n","\n","    return vectorize_filters(filters)\n","\n","\n","def vectorize_filters(filters: dict):\n","    \"\"\"\n","    Function that takes as input the 30x5x5 different SRM high pass filters and creates the 30x3x5x5 tensor with the\n","    following permutations 𝑾𝑗 = [𝑊3𝑘−2 𝑊3𝑘−1 𝑊3𝑘] where 𝑘 = ((𝑗 − 1) mod 10) + 1 and (𝑗 = 1, ⋅ ⋅ ⋅ , 30).\n","    :arg filters: The 30 SRM high pass filters\n","    :return: Returns the 30x3x5x5 filter tensor of the type [number_of_filters, input_channels, height, width]\n","    \"\"\"\n","    tensor_list = []\n","\n","    w = list(filters.values())\n","\n","    for i in range(1, 31):\n","        tmp = []\n","\n","        k = ((i - 1) % 10) + 1\n","\n","        tmp.append(w[3 * k - 3])\n","        tmp.append(w[3 * k - 2])\n","        tmp.append(w[3 * k - 1])\n","\n","        tensor_list.append(stack(tmp))\n","\n","    return stack(tensor_list)"]},{"cell_type":"code","execution_count":17,"metadata":{"gather":{"logged":1724053691603},"id":"-g7TspZFBVte","executionInfo":{"status":"ok","timestamp":1727345864325,"user_tz":-330,"elapsed":3,"user":{"displayName":"KSPSVLN Siddardha Kumar Kavuri","userId":"12765768515309216469"}}},"outputs":[],"source":["def vectorize_filters(filters: dict):\n","    # Function that takes as input the 30x5x5 different SRM high pass filters and creates the 30x3x5x5 tensor with the\n","    # following permutations 𝑾𝑗 = [𝑊3𝑘−2 𝑊3𝑘−1 𝑊3𝑘] where 𝑘 = ((𝑗 − 1) mod 10) + 1 and (𝑗 = 1, ⋅ ⋅ ⋅ , 30).\n","    # :arg filters: The 30 SRM high pass filters\n","    # :return: Returns the 30x3x5x5 filter tensor of the type [number_of_filters, input_channels, height, width]\n","\n","    tensor_list = []\n","\n","    w = list(filters.values())\n","    #print(w)\n","    for i in range(1, 31):\n","        tmp = []\n","\n","        k = ((i - 1) % 10) + 1\n","\n","        tmp.append(w[3 * k - 3])\n","        tmp.append(w[3 * k - 2])\n","        tmp.append(w[3 * k - 1])\n","        # print(3 * k - 1,3 * k - 2,3 * k - 3)\n","        tensor_list.append(stack(tmp))\n","\n","\n","    return stack(tensor_list)"]},{"cell_type":"markdown","metadata":{"editable":true,"id":"yNgafbux8hwd","run_control":{"frozen":false}},"source":["# **Architecture of Computer Neural Network**"]},{"cell_type":"markdown","metadata":{"id":"8iy9SMVmoGn3"},"source":["**Generated model**"]},{"cell_type":"code","execution_count":18,"metadata":{"gather":{"logged":1724051976610},"id":"W4T5z_xNoKB5","executionInfo":{"status":"ok","timestamp":1727345864942,"user_tz":-330,"elapsed":2,"user":{"displayName":"KSPSVLN Siddardha Kumar Kavuri","userId":"12765768515309216469"}}},"outputs":[],"source":["import torch.nn.functional as f\n","import torch.nn as nn\n","\n","\n","\n","class CNN(nn.Module):\n","    \"\"\"\n","    The convolutional neural network (CNN) class\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"\n","        Initialization of all the layers in the network.\n","        \"\"\"\n","        super(CNN, self).__init__()\n","\n","        self.conv0 = nn.Conv2d(3, 3, kernel_size=5, stride=1, padding=0)\n","        nn.init.xavier_uniform_(self.conv0.weight)\n","\n","        self.conv1 = nn.Conv2d(3, 30, kernel_size=5, stride=2, padding=0)\n","        self.conv1.weight = nn.Parameter(get_filters())\n","\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n","\n","        self.conv2 = nn.Conv2d(30, 16, kernel_size=3, stride=1, padding=0)\n","        nn.init.xavier_uniform_(self.conv2.weight)\n","\n","        self.conv3 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=0)\n","        nn.init.xavier_uniform_(self.conv3.weight)\n","\n","        self.conv4 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=0)\n","        nn.init.xavier_uniform_(self.conv4.weight)\n","\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n","\n","        self.conv5 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=0)\n","        nn.init.xavier_uniform_(self.conv5.weight)\n","\n","        self.conv6 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=0)\n","        nn.init.xavier_uniform_(self.conv6.weight)\n","\n","        self.conv7 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=0)\n","        nn.init.xavier_uniform_(self.conv7.weight)\n","\n","        self.conv8 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=0)\n","        nn.init.xavier_uniform_(self.conv8.weight)\n","\n","        self.fc = nn.Linear(16 * 5 * 5, 2)\n","\n","        self.drop1 = nn.Dropout(p=0.5)  # used only for the NC dataset\n","\n","    def forward(self, x):\n","        \"\"\"\n","        The forward step of the network that consumes an image patch and either uses a fully connected layer in the\n","        training phase with a softmax or just returns the feature map after the final convolutional layer.\n","        :returns: Either the output of the softmax during training or the 400-D feature representation at testing\n","        \"\"\"\n","        x = f.relu(self.conv0(x))\n","        x = f.relu(self.conv1(x))\n","        lrn = nn.LocalResponseNorm(3)\n","        x = lrn(x)\n","        x = self.pool1(x)\n","        x = f.relu(self.conv2(x))\n","        x = f.relu(self.conv3(x))\n","        x = f.relu(self.conv4(x))\n","        x = f.relu(self.conv5(x))\n","        x = lrn(x)\n","        x = self.pool2(x)\n","        x = f.relu(self.conv6(x))\n","        x = f.relu(self.conv7(x))\n","        x = f.relu(self.conv8(x))\n","        x = x.view(-1, 16 * 5 * 5)\n","\n","        # In the training phase we also need the fully connected layer with softmax\n","        if self.training:\n","            x = f.relu(self.fc(x))\n","            x = f.softmax(x, dim=1)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"ORt-y5Fk1_h0"},"source":["# **Feature** **Fusion**"]},{"cell_type":"code","execution_count":null,"metadata":{"gather":{"logged":1724047721560},"id":"Eu9fRML919oK"},"outputs":[],"source":["def get_yi(model, patch):\n","    \"\"\"\n","    Returns the patch's feature representation\n","    :param model: The pre-trained CNN object\n","    :param patch: The patch\n","    :returns: The 400-D feature representation of the patch\n","    \"\"\"\n","    with torch.no_grad():\n","        model.eval()\n","        return model(patch)\n","\n","\n","class WrongOperationOption(Exception):\n","    pass\n","\n","\n","def get_y_hat(y: np.ndarray, operation: str):\n","    \"\"\"\n","    Fuses the image's patches feature representation\n","    :param y: The network object\n","    :param operation: Either max or mean for the pooling operation\n","    :returns: The final 400-D feature representation of the entire image\n","    \"\"\"\n","    if operation == \"max\":\n","        return np.array(y).max(axis=0, initial=-math.inf)\n","    elif operation == \"mean\":\n","        return np.array(y).mean(axis=0)\n","    else:\n","        raise WrongOperationOption(\"The operation can be either mean or max\")"]},{"cell_type":"markdown","metadata":{"id":"gIxpPvIQ1Tlm"},"source":["# **Training** **CNN**"]},{"cell_type":"code","execution_count":null,"metadata":{"gather":{"logged":1724053738015},"id":"-PNBNjlQ1ZGi"},"outputs":[],"source":["import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data\n","from torch.optim.lr_scheduler import StepLR\n","from torch.autograd import Variable\n","import time\n","import numpy as np\n","\n","\n","def create_loss_and_optimizer(net, learning_rate=0.01):\n","    \"\"\"\n","    Creates the loss function and optimizer of the network.\n","    :param net: The network object\n","    :param learning_rate: The initial learning rate\n","    :returns: The loss function and the optimizer\n","    \"\"\"\n","    loss = nn.CrossEntropyLoss()\n","    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.99, weight_decay=5 * 1e-4)\n","    return loss, optimizer\n","\n","\n","def train_net(net, train_set, n_epochs, learning_rate, batch_size):\n","    \"\"\"\n","    Training of the CNN\n","    :param net: The CNN object\n","    :param train_set: The training part of the dataset\n","    :param n_epochs: The number of epochs of the experiment\n","    :param learning_rate: The initial learning rate\n","    :param batch_size: The batch size of the SGD\n","    :returns: The epoch loss (vector) and the epoch accuracy (vector)\n","    \"\"\"\n","    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True)\n","    criterion, optimizer = create_loss_and_optimizer(net, learning_rate)\n","    scheduler = StepLR(optimizer, step_size=10, gamma=0.9)\n","    n_batches = len(train_loader)\n","    epoch_loss = []\n","    epoch_accuracy = []\n","\n","    for epoch in range(n_epochs):\n","\n","        total_running_loss = 0.0\n","        print_every = n_batches // 5\n","        training_start_time = time.time()\n","        c = 0\n","        total_predicted = []\n","        total_labels = []\n","\n","        for i, (inputs, labels) in enumerate(train_loader):\n","            # get the inputs\n","            if torch.cuda.is_available():\n","                inputs = Variable(inputs.cuda())\n","                labels = Variable(labels.cuda().long())\n","            else:\n","                inputs = Variable(inputs)\n","                labels = Variable(labels)\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = net(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            _, predicted = torch.max(outputs.data, 1)\n","\n","            total_labels.extend(labels)\n","            total_predicted.extend(predicted)\n","\n","            if (i + 1) % (print_every + 1) == 0:\n","                total_running_loss += loss.item()\n","                c += 1\n","\n","        epoch_predictions = (np.array(total_predicted) == np.array(total_labels)).sum().item()\n","        print('---------- Epoch %d Loss: %.3f Accuracy: %.3f Time: %.3f----------' % (\n","            epoch + 1, total_running_loss / c, epoch_predictions / len(total_predicted),\n","            time.time() - training_start_time))\n","        epoch_accuracy.append(epoch_predictions / len(total_predicted))\n","        epoch_loss.append(total_running_loss / c)\n","        scheduler.step()\n","        pd.DataFrame(epoch_loss).to_csv('patches/reports/SRM_loss.csv')\n","        pd.DataFrame(epoch_accuracy).to_csv('patches/reports/SRM_accuracy.csv')\n","\n","        torch.save(net, 'patches/model/Cnn.pt')\n","    print('Finished Training')\n","\n","    return epoch_loss, epoch_accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"gather":{"logged":1724053626185},"id":"-AxQLf19szTA","jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["torch.manual_seed(0)\n","# put the directory of the patches in your machine\n","transform = transforms.Compose([transforms.ToTensor()])\n","DATA_DIR='patches/patches_casia_with_rot'\n","data = datasets.ImageFolder(root=DATA_DIR, transform=transform)  # Fetch data\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")#10151\n","\n","if str(device) == \"cuda:0\":\n","    print(\"cuda enabled\")\n","    cnn = torch.load('patches/model/Cnn.pt').cuda()\n","else:\n","    print(\"no cuda\")\n","    cnn = torch.load('patches/model/Cnn.pt')\n","\n","epoch_loss, epoch_accuracy = train_net(cnn, data, n_epochs=50, learning_rate=0.0001, batch_size=128)\n","\n","pd.DataFrame(epoch_loss).to_csv('patches/reports/SRM_loss.csv')\n","pd.DataFrame(epoch_accuracy).to_csv('patches/reports/SRM_accuracy.csv')\n","\n","torch.save(cnn.state_dict(), 'patches/model/Cnn1.pt')\n","torch.save(cnn, 'patches/model/Cnn.pt')"]},{"cell_type":"markdown","metadata":{"id":"9eHGi9xU2XAZ"},"source":["# **Feature Vector Generator / Training SVM**"]},{"cell_type":"code","execution_count":13,"metadata":{"gather":{"logged":1724035947898},"id":"QlGp55aA2nIM","executionInfo":{"status":"ok","timestamp":1727345850768,"user_tz":-330,"elapsed":1260,"user":{"displayName":"KSPSVLN Siddardha Kumar Kavuri","userId":"12765768515309216469"}}},"outputs":[],"source":["import glob\n","import cv2\n","def get_patches(image_mat, stride):\n","    \"\"\"\n","    Extract patches rom an image\n","    :param image_mat: The image as a matrix\n","    :param stride: The stride of the patch extraction process\n","    :returns: The patches\n","    \"\"\"\n","    window_shape = (128, 128, 3)\n","    windows = view_as_windows(image_mat, window_shape, step=stride)\n","    patches = []\n","    for m in range(windows.shape[0]):\n","        for n in range(windows.shape[1]):\n","            patches += [windows[m][n][0]]\n","    return patches\n","\n","\n","def get_images_and_labels(tampered_path, authentic_path):\n","    \"\"\"\n","    Get the images and their corresponding labels\n","    :param tampered_path: The path containing the tampered images\n","    :param authentic_path: The path containing the authentic images\n","    :returns: Dictionary with images and labels\n","    \"\"\"\n","    tampered_dir = tampered_path\n","    authentic_dir = authentic_path\n","    images = {}\n","    for im in glob.glob(authentic_dir):\n","        images[im] = {}\n","        images[im]['mat'] = cv2.imread(im)\n","        images[im]['label'] = 0\n","    for im in glob.glob(tampered_dir):\n","        images[im] = {}\n","        images[im]['mat'] = cv2.imread(im)\n","        images[im]['label'] = 1\n","    return images\n"]},{"cell_type":"code","execution_count":14,"metadata":{"gather":{"logged":1724047747371},"id":"11sY8wUY2Rif","executionInfo":{"status":"ok","timestamp":1727345851454,"user_tz":-330,"elapsed":10,"user":{"displayName":"KSPSVLN Siddardha Kumar Kavuri","userId":"12765768515309216469"}}},"outputs":[],"source":["def create_feature_vectors(model, tampered_path, authentic_path, output_name):\n","    \"\"\"\n","    Writes the feature vectors of the CASIA2 dataset.\n","    :param model: The pre-trained CNN object\n","    :param tampered_path: The path of the tampered images of the CASIA2 dataset\n","    :param authentic_path: The path of the authentic images of the CASIA2 dataset\n","    :param output_name: The name of the output CSV that contains the feature vectors\n","    \"\"\"\n","    df = pd.DataFrame()\n","    images = get_images_and_labels(tampered_path, authentic_path)\n","    c = 1\n","    for image_name in images.keys():  # images\n","        print(\"Image: \", c)\n","\n","        image = images[image_name]['mat']\n","        label = images[image_name]['label']\n","\n","        df = pd.concat([df, pd.concat([pd.DataFrame([image_name.split(os.sep)[-1], str(label)]),\n","                                       pd.DataFrame(get_patch_yi(model, image))])], axis=1, sort=False)\n","        c += 1\n","\n","    # save the feature vector to csv\n","    final_df = df.T\n","    final_df.columns = get_df_column_names()\n","    final_df.to_csv(output_name, index=False)  # csv type [im_name][label][f1,f2,...,fK]\n","\n","\n","def get_patch_yi(model, image):\n","    \"\"\"\n","    Calculates the feature representation of an image.\n","    :param model: The pre-trained CNN object\n","    :param image: The image\n","    :returns: The image's feature representation\n","    \"\"\"\n","    transform = transforms.Compose([transforms.ToTensor()])\n","\n","    y = []  # init Y\n","\n","    patches = get_patches(image, stride=1024)\n","\n","    for patch in patches:  # for every patch\n","        img_tensor = transform(patch)\n","        img_tensor.unsqueeze_(0)\n","        img_variable = Variable(img_tensor.double())\n","        yi = get_yi(model=model, patch=img_variable)\n","        y.append(yi)  # append Yi to Y\n","\n","    y = np.vstack(tuple(y))\n","\n","    y_hat = get_y_hat(y=y, operation=\"mean\")  # create Y_hat with mean or max\n","\n","    return y_hat\n","\n","\n","def get_df_column_names():\n","    \"\"\"\n","    Rename the feature csv column names as [im_names][labels][f1,f2,...,fK].\n","    :returns: The column names\n","    \"\"\"\n","    names = [\"image_names\", \"labels\"]\n","    for i in range(400):\n","        names.append(\"f\" + str(i + 1))\n","    return names"]},{"cell_type":"code","execution_count":19,"metadata":{"gather":{"logged":1724049719182},"id":"UTgR5oNk3dgR","colab":{"base_uri":"https://localhost:8080/","height":413},"executionInfo":{"status":"error","timestamp":1727345870279,"user_tz":-330,"elapsed":663,"user":{"displayName":"KSPSVLN Siddardha Kumar Kavuri","userId":"12765768515309216469"}},"outputId":"ae69ff13-0bb0-404f-a586-ce8ad146bc08"},"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'1O1'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-e08e79dba20d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Move model to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive/MyDrive/summer/Final/data/output/pre_trained_cnn/CASIA2_WithRot_LR001_b128_nodrop.pt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-a8ef04a95c0e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_filters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-acfc0209bb66>\u001b[0m in \u001b[0;36mget_filters\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     filters[\"11\"] = Tensor(np.array([[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, -1, 1, 0], [0, 0, 0, 0, 0],\n\u001b[1;32m     18\u001b[0m                                       [0, 0, 0, 0, 0]]))\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mfilters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"12\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrot90\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"1O1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mfilters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"13\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrot90\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"1O2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mfilters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"14\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrot90\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"1O3\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: '1O1'"]}],"source":["import torch\n","with torch.no_grad():\n","    model = CNN() # Move model to GPU\n","\n","    model.load_state_dict(torch.load('drive/MyDrive/summer/Final/data/output/pre_trained_cnn/CASIA2_WithRot_LR001_b128_nodrop.pt',map_location=torch.device('cpu')))\n","    model.eval()\n","    model = model.double()\n","\n","    authentic_path = 'drive/MyDrive/summer/Final/CASIA2.0_revised/Au/*'\n","    tampered_path = 'drive/MyDrive/summer/Final/CASIA2.0_revised/Tp/*'\n","    output_filename = 'drive/MyDrive/summer/Final/data/output/loss_function/CASIA2_WithRot_LR001_b128_nodrop.csv'\n","    create_feature_vectors(model, tampered_path, authentic_path, output_filename)"]},{"cell_type":"markdown","metadata":{"id":"Bj4XZUjL290Y"},"source":["# **Saving SVM Classifier**"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Zbg5qq753AUj","executionInfo":{"status":"ok","timestamp":1727345187010,"user_tz":-330,"elapsed":882,"user":{"displayName":"KSPSVLN Siddardha Kumar Kavuri","userId":"12765768515309216469"}}},"outputs":[],"source":["def optimize_hyperparams(x, y, params):\n","    \"\"\"\n","    Hyperparameter optimization of the SVM\n","    :param x: The feature vectors\n","    :param y: The labels\n","    :param params: The grid of all the possible optimal hyperparameters\n","    :returns: The optimal hyperparameters\n","    \"\"\"\n","    # Optimize hyper-parameters\n","    model = svm.SVC()\n","    model_grid_search = GridSearchCV(model, params, cv=10, n_jobs=5)\n","    model_grid_search.fit(x.values, y.values)\n","    print(\"Optimal hyper-parameters: \", model_grid_search.best_params_)\n","    print(\"Accuracy :\", model_grid_search.best_score_)\n","    return model_grid_search.best_params_\n","\n","\n","def classify(x, y, opt_params):\n","    \"\"\"\n","    Classify a feature vector using SVM and print some metrics\n","    :param x: The feature vectors\n","    :param y: The labels\n","    :param opt_params: The optimal hyperparameters\n","    \"\"\"\n","    # Single SVM run with optimized hyperparameters and\n","    model = svm.SVC(kernel='rbf', gamma=opt_params['gamma'], C=opt_params['C'])\n","    scores = cross_val_score(model, x, y, cv=10, scoring='accuracy', n_jobs=-1)\n","    print(scores)\n","    print(np.mean(scores))\n","    print(np.std(scores))\n","\n","\n","def print_confusion_matrix(x, y, opt_params):\n","    \"\"\"\n","    Print the confusion matrix of an SVM classification\n","    :param x: The feature vectors\n","    :param y: The labels\n","    :param opt_params: The optimal hyperparameters\n","    \"\"\"\n","    y_pred, y_test = get_predictions(x, y, opt_params)\n","    # Printing out false/true positives/negatives\n","    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n","    print('True negatives: ', tn, 'False positives: ', fp, 'False negatives: ', fn, 'True positives: ', tp)\n","\n","    # Using seaborn to create a confusion matrix table\n","    data = {'y_Predicted': y_pred, 'y_Actual': y_test}\n","    df = pd.DataFrame(data, columns=['y_Actual', 'y_Predicted'])\n","    conf_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n","    sn.heatmap(conf_matrix, cmap=ListedColormap(['#ED7D31', '#009FDA']), annot=True, fmt='g', cbar=False)\n","\n","\n","def find_misclassified(x, y, opt_params, img_ids):\n","    \"\"\"\n","    Gets the misclassified image ids and writes them to a csv\n","    :param x: The feature vectors\n","    :param y: The labels\n","    :param opt_params: The optimal hyperparameters\n","    :param img_ids: The image ids that correspond to the feature vector(x)\n","    \"\"\"\n","    y_pred, y_test = get_predictions(x, y, opt_params)\n","    misclassified = []\n","    for i in range(len(y_test)):\n","        if y_pred[i] != y_test.values[i]:\n","            misclassified.append(str(y_pred[i]) + ',' + str(y_test.values[i]) + ',' + str(img_ids[y_test.index[i]]))\n","    df = pd.DataFrame(misclassified)\n","    df.columns = ['Prediction,Actual,ImageName']\n","    df.to_csv('Misclassified.csv', index=False)\n","\n","\n","def get_predictions(x, y, opt_params):\n","    \"\"\"\n","    Classification using SVM\n","    :param x: The feature vectors\n","    :param y: The labels\n","    :param opt_params: The optimal hyperparameters\n","    :returns: The predicted and true labels\n","    \"\"\"\n","    # Run one SVM with 80-20 split\n","    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=0)\n","    model = svm.SVC(kernel='rbf', gamma=opt_params['gamma'], C=opt_params['C'])\n","    model.fit(x_train, y_train)\n","    y_pred = model.predict(x_test)\n","\n","    return y_pred, y_test"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"nNGoREcd3NoZ","colab":{"base_uri":"https://localhost:8080/","height":412},"executionInfo":{"status":"error","timestamp":1727345692596,"user_tz":-330,"elapsed":519,"user":{"displayName":"KSPSVLN Siddardha Kumar Kavuri","userId":"12765768515309216469"}},"outputId":"e9d88c27-abdc-4ec9-bf95-5e03c0464812"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'drive/Mydrive/summer/Final/data/output/pre_trained_cnn/CASIA2_WithRot_LR001_b128_nodrop.pt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-73d34b49268e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read features and labels from CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'drive/Mydrive/summer/Final/data/output/pre_trained_cnn/CASIA2_WithRot_LR001_b128_nodrop.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'image_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/Mydrive/summer/Final/data/output/pre_trained_cnn/CASIA2_WithRot_LR001_b128_nodrop.pt'"]}],"source":["# Read features and labels from CSV\n","df = pd.read_csv(filepath_or_buffer='drive/Mydrive/summer/Final/data/output/pre_trained_cnn/CASIA2_WithRot_LR001_b128_nodrop.pt')\n","X = df.loc[:, ~df.columns.isin(['labels', 'image_names'])]\n","y = df['labels']\n","\n","img_ids = df['image_names']\n","\n","print('Has NaN:', df.isnull().values.any())\n","\n","hyper_params = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}]\n","\n","opt_params = optimize_hyperparams(X, y, params=hyper_params)\n","classify(X, y, opt_params)\n","print_confusion_matrix(X, y, opt_params)\n","find_misclassified(X, y, opt_params, img_ids)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"QTYT7knbygKe","executionInfo":{"status":"aborted","timestamp":1727345188331,"user_tz":-330,"elapsed":6,"user":{"displayName":"KSPSVLN Siddardha Kumar Kavuri","userId":"12765768515309216469"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","availableInstances":[{"_defaultOrder":0,"_isFastLaunch":true,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":4,"name":"ml.t3.medium","vcpuNum":2},{"_defaultOrder":1,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.t3.large","vcpuNum":2},{"_defaultOrder":2,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.t3.xlarge","vcpuNum":4},{"_defaultOrder":3,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.t3.2xlarge","vcpuNum":8},{"_defaultOrder":4,"_isFastLaunch":true,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.m5.large","vcpuNum":2},{"_defaultOrder":5,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.m5.xlarge","vcpuNum":4},{"_defaultOrder":6,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.m5.2xlarge","vcpuNum":8},{"_defaultOrder":7,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.m5.4xlarge","vcpuNum":16},{"_defaultOrder":8,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.m5.8xlarge","vcpuNum":32},{"_defaultOrder":9,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.m5.12xlarge","vcpuNum":48},{"_defaultOrder":10,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.m5.16xlarge","vcpuNum":64},{"_defaultOrder":11,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.m5.24xlarge","vcpuNum":96},{"_defaultOrder":12,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.m5d.large","vcpuNum":2},{"_defaultOrder":13,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.m5d.xlarge","vcpuNum":4},{"_defaultOrder":14,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.m5d.2xlarge","vcpuNum":8},{"_defaultOrder":15,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.m5d.4xlarge","vcpuNum":16},{"_defaultOrder":16,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.m5d.8xlarge","vcpuNum":32},{"_defaultOrder":17,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.m5d.12xlarge","vcpuNum":48},{"_defaultOrder":18,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.m5d.16xlarge","vcpuNum":64},{"_defaultOrder":19,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.m5d.24xlarge","vcpuNum":96},{"_defaultOrder":20,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":true,"memoryGiB":0,"name":"ml.geospatial.interactive","supportedImageNames":["sagemaker-geospatial-v1-0"],"vcpuNum":0},{"_defaultOrder":21,"_isFastLaunch":true,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":4,"name":"ml.c5.large","vcpuNum":2},{"_defaultOrder":22,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.c5.xlarge","vcpuNum":4},{"_defaultOrder":23,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.c5.2xlarge","vcpuNum":8},{"_defaultOrder":24,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.c5.4xlarge","vcpuNum":16},{"_defaultOrder":25,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":72,"name":"ml.c5.9xlarge","vcpuNum":36},{"_defaultOrder":26,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":96,"name":"ml.c5.12xlarge","vcpuNum":48},{"_defaultOrder":27,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":144,"name":"ml.c5.18xlarge","vcpuNum":72},{"_defaultOrder":28,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.c5.24xlarge","vcpuNum":96},{"_defaultOrder":29,"_isFastLaunch":true,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.g4dn.xlarge","vcpuNum":4},{"_defaultOrder":30,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.g4dn.2xlarge","vcpuNum":8},{"_defaultOrder":31,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.g4dn.4xlarge","vcpuNum":16},{"_defaultOrder":32,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.g4dn.8xlarge","vcpuNum":32},{"_defaultOrder":33,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.g4dn.12xlarge","vcpuNum":48},{"_defaultOrder":34,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.g4dn.16xlarge","vcpuNum":64},{"_defaultOrder":35,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":61,"name":"ml.p3.2xlarge","vcpuNum":8},{"_defaultOrder":36,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":244,"name":"ml.p3.8xlarge","vcpuNum":32},{"_defaultOrder":37,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":488,"name":"ml.p3.16xlarge","vcpuNum":64},{"_defaultOrder":38,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.p3dn.24xlarge","vcpuNum":96},{"_defaultOrder":39,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.r5.large","vcpuNum":2},{"_defaultOrder":40,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.r5.xlarge","vcpuNum":4},{"_defaultOrder":41,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.r5.2xlarge","vcpuNum":8},{"_defaultOrder":42,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.r5.4xlarge","vcpuNum":16},{"_defaultOrder":43,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.r5.8xlarge","vcpuNum":32},{"_defaultOrder":44,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.r5.12xlarge","vcpuNum":48},{"_defaultOrder":45,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.r5.16xlarge","vcpuNum":64},{"_defaultOrder":46,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.r5.24xlarge","vcpuNum":96},{"_defaultOrder":47,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.g5.xlarge","vcpuNum":4},{"_defaultOrder":48,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.g5.2xlarge","vcpuNum":8},{"_defaultOrder":49,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.g5.4xlarge","vcpuNum":16},{"_defaultOrder":50,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.g5.8xlarge","vcpuNum":32},{"_defaultOrder":51,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.g5.16xlarge","vcpuNum":64},{"_defaultOrder":52,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.g5.12xlarge","vcpuNum":48},{"_defaultOrder":53,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.g5.24xlarge","vcpuNum":96},{"_defaultOrder":54,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.g5.48xlarge","vcpuNum":192},{"_defaultOrder":55,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":1152,"name":"ml.p4d.24xlarge","vcpuNum":96},{"_defaultOrder":56,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":1152,"name":"ml.p4de.24xlarge","vcpuNum":96},{"_defaultOrder":57,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.trn1.2xlarge","vcpuNum":8},{"_defaultOrder":58,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.trn1.32xlarge","vcpuNum":128},{"_defaultOrder":59,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.trn1n.32xlarge","vcpuNum":128}],"colab":{"collapsed_sections":["fuBGU_ddPASi","QY_DLIMF843J","3U23mBUfsmr2","AmvmJapo1PnK","yNgafbux8hwd","ORt-y5Fk1_h0","gIxpPvIQ1Tlm","9eHGi9xU2XAZ","Bj4XZUjL290Y"],"gpuType":"T4","provenance":[{"file_id":"1mWSV-q8J5iX9nxQmQqKqiINjhMJYDBfG","timestamp":1724203202637}]},"instance_type":"ml.t3.medium","kernel_info":{"name":"python310-sdkv2"},"kernelspec":{"display_name":"Python 3 (Data Science 3.0)","language":"python","name":"python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-south-1:394103062818:image/sagemaker-data-science-310-v1"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"microsoft":{"ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"toc-autonumbering":true,"toc-showcode":true,"toc-showmarkdowntxt":false},"nbformat":4,"nbformat_minor":0}